{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a986e1d5",
   "metadata": {},
   "source": [
    "## Creating the SQLite Databases Necessary for DES Sorcha\n",
    "\n",
    "This notebook creates the SQLite databases needed to run **DES** in Sorcha. This code obtains the nescessory data from https://github.com/bernardinelli/DESTNOSIM/tree/master/data and converts it to sqlite databases. It creates two files:\n",
    "\n",
    "1. **Visits database** from `y6a1c.ccdcorners.fits.gz`\n",
    "2. **Pointing database** from `y6a1c.exposures.positions.fits`\n",
    "\n",
    "---\n",
    "For running DES, obtain the databases using this notebook, run the command line arg `sorcha init` and select the **DES config file** option. Then run the code with:\n",
    "\n",
    "sorcha run -c DES_config_file.ini --pd DES_TNO.db --ob orbits_filename.csv -p colours_filename.csv -o ./ -s des --vd visits_DES.db\n",
    "\n",
    "`orbits_filename.csv` and `colours_filename.csv` should be your input files for object's orbits and colours, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc0f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DES pointing data processed and saved to: /Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_TNO.db\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Big-endian buffer not supported on little-endian compiler",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 211\u001b[39m\n\u001b[32m    208\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDES CCD visits data saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    210\u001b[39m process_pointings_to_sqlite(\u001b[38;5;28;01mTrue\u001b[39;00m,db_path=\u001b[33m\"\u001b[39m\u001b[33m/Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_TNO.db\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m \u001b[43mprocess_ccd_visits_to_sqlite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdb_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_visits.db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpointings_db_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_TNO.db\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 194\u001b[39m, in \u001b[36mprocess_ccd_visits_to_sqlite\u001b[39m\u001b[34m(use_url, fits_path, db_path, pointings_db_path, url)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;66;03m# Merge on visitId = observationId\u001b[39;00m\n\u001b[32m    193\u001b[39m df_ccd = df_ccd.merge(df_depth, left_on=\u001b[33m\"\u001b[39m\u001b[33mvisitId\u001b[39m\u001b[33m\"\u001b[39m, right_on=\u001b[33m\"\u001b[39m\u001b[33mobservationId\u001b[39m\u001b[33m\"\u001b[39m, how=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[43mdf_ccd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobservationId\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    197\u001b[39m engine = create_engine(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33msqlite:///\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    198\u001b[39m df_ccd.to_sql(\u001b[33m\"\u001b[39m\u001b[33mobservations\u001b[39m\u001b[33m\"\u001b[39m, engine, if_exists=\u001b[33m\"\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m, chunksize=\u001b[32m10000\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/frame.py:5581\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5433\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5434\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5435\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5442\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5443\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5444\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5445\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5446\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5579\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5580\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5581\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5582\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5583\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5587\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5588\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5589\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/generic.py:4788\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4786\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4787\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4788\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4790\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4791\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/generic.py:4869\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4866\u001b[39m     new_axis = axis.take(indexer)\n\u001b[32m   4868\u001b[39m bm_axis = \u001b[38;5;28mself\u001b[39m.ndim - axis_num - \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m4869\u001b[39m new_mgr = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreindex_indexer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4870\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4871\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4872\u001b[39m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4873\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_dups\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4874\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4875\u001b[39m \u001b[43m    \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4876\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4877\u001b[39m result = \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(new_mgr, axes=new_mgr.axes)\n\u001b[32m   4878\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim == \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/internals/managers.py:680\u001b[39m, in \u001b[36mBaseBlockManager.reindex_indexer\u001b[39m\u001b[34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, only_slice, use_na_proxy)\u001b[39m\n\u001b[32m    677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mRequested axis not found in manager\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m axis == \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m680\u001b[39m     new_blocks = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_slice_take_blocks_ax0\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    681\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    682\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[43m        \u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m=\u001b[49m\u001b[43monly_slice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    684\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_na_proxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    685\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    686\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    687\u001b[39m     new_blocks = [\n\u001b[32m    688\u001b[39m         blk.take_nd(\n\u001b[32m    689\u001b[39m             indexer,\n\u001b[32m   (...)\u001b[39m\u001b[32m    695\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blocks\n\u001b[32m    696\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/internals/managers.py:843\u001b[39m, in \u001b[36mBaseBlockManager._slice_take_blocks_ax0\u001b[39m\u001b[34m(self, slice_or_indexer, fill_value, only_slice, use_na_proxy, ref_inplace_op)\u001b[39m\n\u001b[32m    841\u001b[39m                     blocks.append(nb)\n\u001b[32m    842\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m843\u001b[39m                 nb = \u001b[43mblk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtaker\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_mgr_locs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmgr_locs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    844\u001b[39m                 blocks.append(nb)\n\u001b[32m    846\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m blocks\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/internals/blocks.py:1307\u001b[39m, in \u001b[36mBlock.take_nd\u001b[39m\u001b[34m(self, indexer, axis, new_mgr_locs, fill_value)\u001b[39m\n\u001b[32m   1304\u001b[39m     allow_fill = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1306\u001b[39m \u001b[38;5;66;03m# Note: algos.take_nd has upcast logic similar to coerce_to_target_dtype\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m new_values = \u001b[43malgos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtake_nd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfill_value\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1311\u001b[39m \u001b[38;5;66;03m# Called from three places in managers, all of which satisfy\u001b[39;00m\n\u001b[32m   1312\u001b[39m \u001b[38;5;66;03m#  these assertions\u001b[39;00m\n\u001b[32m   1313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ExtensionBlock):\n\u001b[32m   1314\u001b[39m     \u001b[38;5;66;03m# NB: in this case, the 'axis' kwarg will be ignored in the\u001b[39;00m\n\u001b[32m   1315\u001b[39m     \u001b[38;5;66;03m#  algos.take_nd call above.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/array_algos/take.py:117\u001b[39m, in \u001b[36mtake_nd\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.take(indexer, fill_value=fill_value, allow_fill=allow_fill)\n\u001b[32m    116\u001b[39m arr = np.asarray(arr)\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_take_nd_ndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_fill\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/sorcha/lib/python3.11/site-packages/pandas/core/array_algos/take.py:162\u001b[39m, in \u001b[36m_take_nd_ndarray\u001b[39m\u001b[34m(arr, indexer, axis, fill_value, allow_fill)\u001b[39m\n\u001b[32m    157\u001b[39m     out = np.empty(out_shape, dtype=dtype)\n\u001b[32m    159\u001b[39m func = _get_take_nd_function(\n\u001b[32m    160\u001b[39m     arr.ndim, arr.dtype, out.dtype, axis=axis, mask_info=mask_info\n\u001b[32m    161\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m flip_order:\n\u001b[32m    165\u001b[39m     out = out.T\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/algos_take_helper.pxi:1209\u001b[39m, in \u001b[36mpandas._libs.algos.take_2d_axis1_int32_int32\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mValueError\u001b[39m: Big-endian buffer not supported on little-endian compiler"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "from astropy.time import Time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "\n",
    "\n",
    "\n",
    "def process_pointings_to_sqlite(\n",
    "    use_url,\n",
    "    fits_path=\"y6a1c.exposures.positions.fits\",\n",
    "    db_path=\"DES_TNO.db\",\n",
    "    url=\"https://github.com/bernardinelli/DESTNOSIM/raw/refs/heads/master/data/y6a1c.exposures.positions.fits\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a FITS file containing DES pointing data and stores \n",
    "    data into a SQLite database with indexing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_url : bool\n",
    "        If True, the FITS file will be downloaded from the specified `url`. If False, the local file path\n",
    "        specified by `fits_path` will be used.\n",
    "\n",
    "    fits_path : str, optional\n",
    "        Path to the local FITS file (default is \"y6a1c.exposures.positions.fits\"). Only used if `use_url` is False.\n",
    "\n",
    "    db_path : str, optional\n",
    "        Path where the SQLite database will be saved (default is \"DES_TNO.db\").\n",
    "\n",
    "    url : str, optional\n",
    "        Direct URL to the raw FITS file hosted on GitHub.\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    if use_url == True:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with fits.open(BytesIO(response.content)) as hdul:\n",
    "            df = hdul[1].data\n",
    "    else:\n",
    "        with fits.open(fits_path) as HDUl:\n",
    "            df = HDUl[1].data\n",
    "\n",
    "    # Define column names\n",
    "    cov_1 = \"cov_xx\"\n",
    "    cov_2 = \"cov_yy\"\n",
    "    cov_3 = \"cov_xy\"\n",
    "    observatory_1 = \"observatory_1\"\n",
    "    observatory_2 = \"observatory_2\"\n",
    "    observatory_3 = \"observatory_3\"\n",
    "    velocity_1 = \"velocity_1\"\n",
    "    velocity_2 = \"velocity_2\"\n",
    "    velocity_3 = \"velocity_3\"\n",
    "\n",
    "    # Create dictionary for DataFrame construction\n",
    "    temp = {\n",
    "        cov_1: np.array(df[\"cov\"][:, 0], dtype=\"<f8\"),\n",
    "        cov_2: np.array(df[\"cov\"][:, 1], dtype=\"<f8\"),\n",
    "        cov_3: np.array(df[\"cov\"][:, 2], dtype=\"<f8\"),\n",
    "        \"covwarn\": df[\"covwarn\"],\n",
    "        \"fieldDec\": np.array(df[\"dec\"], dtype=\"<f8\"),\n",
    "        \"ecl_lat\": np.array(df[\"ecl_lat\"], dtype=\"<f8\"),\n",
    "        \"ecl_lon\": np.array(df[\"ecl_lon\"], dtype=\"<f8\"),\n",
    "        \"observationId\": np.array(df[\"expnum\"], \"<i4\"),\n",
    "        \"filter\": df[\"filter\"],\n",
    "        \"observationMidpointMJD\": np.array(df[\"mjd_mid\"], dtype=\"<f8\"),\n",
    "        observatory_1: np.array(df[\"observatory\"][:, 0], dtype=\"<f8\"),\n",
    "        observatory_2: np.array(df[\"observatory\"][:, 1], dtype=\"<f8\"),\n",
    "        observatory_3: np.array(df[\"observatory\"][:, 2], dtype=\"<f8\"),\n",
    "        \"obs_ecl_lon\": np.array(df[\"obs_ecl_lon\"], \"<f8\"),\n",
    "        \"fieldRA\": np.array(df[\"ra\"], dtype=\"<f8\"),\n",
    "        \"fiveSigmaDepth\": np.array(df[\"m50\"], dtype=\"<f8\"),\n",
    "        \"k\": np.array(df[\"k\"], dtype=\"<f8\"),\n",
    "        \"c\": np.array(df[\"c\"], dtype=\"<f8\"),\n",
    "        velocity_1: np.array(df[\"velocity\"][:, 0], dtype=\"<f8\"),\n",
    "        velocity_2: np.array(df[\"velocity\"][:, 1], dtype=\"<f8\"),\n",
    "        velocity_3: np.array(df[\"velocity\"][:, 2], dtype=\"<f8\"),\n",
    "    }\n",
    "\n",
    "    # Set exposure times\n",
    "    exo_time_s = np.full(len(df), 90)\n",
    "    mask = (temp[\"observationMidpointMJD\"] < 57447) & (temp[\"filter\"] == \"Y\")\n",
    "    exo_time_s[mask] = 45\n",
    "\n",
    "    # Convert TDB to TAI\n",
    "    time = Time(temp[\"observationMidpointMJD\"], format=\"mjd\", scale=\"tdb\")\n",
    "    time_TAI = time.tai\n",
    "    temp[\"observationMidpointMJD\"] = time_TAI.value\n",
    "\n",
    "    # Check for NaNs\n",
    "    if np.any(pd.isnull(temp[\"observationMidpointMJD\"])):\n",
    "        print(\"Warning: NaN values found in observationMidpointMJD\")\n",
    "\n",
    "    # Create DataFrame and insert exposure times\n",
    "    df_hdl1 = pd.DataFrame(temp)\n",
    "    df_hdl1.insert(8, \"visitExposureTime\", exo_time_s)\n",
    "\n",
    "    # Save to SQLite database\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    df_hdl1.to_sql(\"observations\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Create indexes efficiently\n",
    "    index_queries = [\n",
    "        \"CREATE INDEX idx_lat_long ON observations(ecl_lat,ecl_lon)\",\n",
    "        \"CREATE INDEX idx_filter ON observations(filter)\",\n",
    "        \"CREATE INDEX idx_dec_ra ON observations(fieldDec,fieldRA)\",\n",
    "        \"CREATE INDEX idx_dec_ra_mjd ON observations(fieldDec,fieldRA,observationMidpointMJD)\",\n",
    "        \"CREATE INDEX idx_mjd ON observations(observationMidpointMJD)\",\n",
    "        \"CREATE INDEX idx_m50_c_k ON observations(fiveSigmaDepth,c,k)\"\n",
    "    ]\n",
    "\n",
    "    with sqlite3.connect(db_path) as db:\n",
    "        cursor = db.cursor()\n",
    "        for query in index_queries:\n",
    "            cursor.execute(query)\n",
    "\n",
    "    print(f\"DES pointing data processed and saved to: {db_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_ccd_visits_to_sqlite(\n",
    "    use_url,\n",
    "    fits_path=\"y6a1c.ccdcorners.fits.gz\",\n",
    "    db_path=\"visits_DES.db\",\n",
    "    pointings_db_path=\"DES_TNO.db\",\n",
    "    url=\"https://github.com/bernardinelli/DESTNOSIM/raw/refs/heads/master/data/y6a1c.ccdcorners.fits.gz\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes DES CCD corners FITS data and joins it with the fiveSigmaDepth from pointings data.\n",
    "    Saves result to a SQLite database with spatial indexing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_url : bool\n",
    "        Whether to download the FITS from the URL or use a local path.\n",
    "\n",
    "    fits_path : str\n",
    "        Local FITS file path if not using URL.\n",
    "\n",
    "    db_path : str\n",
    "        Output SQLite path.\n",
    "\n",
    "    pointings_db_path : str\n",
    "        Path to SQLite file containing pointings (to pull fiveSigmaDepth).\n",
    "\n",
    "    url : str\n",
    "        URL to the FITS file (gzipped).\n",
    "    \"\"\"\n",
    "\n",
    "    if use_url:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with gzip.open(BytesIO(response.content), 'rb') as gz:\n",
    "            with fits.open(gz) as hdul:\n",
    "                df = hdul[1].data\n",
    "                df = df.byteswap().newbyteorder()\n",
    "    else:\n",
    "        with fits.open(fits_path) as hdul:\n",
    "            df = hdul[1].data\n",
    "            df = df.byteswap().newbyteorder()\n",
    "\n",
    "    ra = df[\"ra\"]\n",
    "    dec = df[\"dec\"]\n",
    "\n",
    "    temp = {\n",
    "        \"visitId\": df[\"expnum\"],\n",
    "        \"detectorID\": df[\"ccdnum\"],\n",
    "        \"llcra\": ra[:, 0],\n",
    "        \"llcdec\": dec[:, 0],\n",
    "        \"lrcra\": ra[:, 1],\n",
    "        \"lrcdec\": dec[:, 1],\n",
    "        \"urcra\": ra[:, 2],\n",
    "        \"urcdec\": dec[:, 2],\n",
    "        \"ulcra\": ra[:, 3],\n",
    "        \"ulcdec\": dec[:, 3],\n",
    "        \"ra\": ra[:, 4],   # center RA\n",
    "        \"dec\": dec[:, 4], # center Dec\n",
    "    }\n",
    "\n",
    "    df_ccd = pd.DataFrame(temp)\n",
    "\n",
    "\n",
    "    engine_pointings = create_engine(f\"sqlite:///{pointings_db_path}\")\n",
    "    df_depth = pd.read_sql(\"SELECT observationId, fiveSigmaDepth FROM observations\", engine_pointings)\n",
    "    df_depth[\"observationId\"] = df_depth[\"observationId\"].astype(\"<i4\").copy()\n",
    "\n",
    "    # Merge on visitId = observationId\n",
    "    df_ccd = df_ccd.merge(df_depth, left_on=\"visitId\", right_on=\"observationId\", how=\"left\")\n",
    "    df_ccd.drop(columns=[\"observationId\"], inplace=True)\n",
    "\n",
    "\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    df_ccd.to_sql(\"observations\", engine, if_exists=\"replace\", index=False, chunksize=10000)\n",
    "\n",
    "    index_sql = \"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_obs \n",
    "    ON observations(visitId, llcra, llcdec, lrcra, lrcdec, urcra, urcdec, ulcra, ulcdec)\n",
    "    \"\"\"\n",
    "\n",
    "    with sqlite3.connect(db_path) as db:\n",
    "        db.execute(index_sql)\n",
    "\n",
    "    print(f\"DES CCD visits data saved to {db_path}\")\n",
    "\n",
    "process_pointings_to_sqlite(True,db_path=\"/Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_TNO.db\")\n",
    "process_ccd_visits_to_sqlite(True, db_path=\"/Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_visits.db\", pointings_db_path=\"/Users/ryanlyttle/Documents/GitHub/sorcha/testing_des/DES_TNO.db\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sorcha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
