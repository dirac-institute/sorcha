{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a986e1d5",
   "metadata": {},
   "source": [
    "## Creating the SQLite Databases Necessary for DES Sorcha\n",
    "\n",
    "This notebook creates the SQLite databases needed to run **DES** in Sorcha. This code obtains the nescessory data from https://github.com/bernardinelli/DESTNOSIM/tree/master/data and converts it to sqlite databases. It creates two files:\n",
    "\n",
    "1. **Visits database** from `y6a1c.ccdcorners.fits.gz`\n",
    "2. **Pointing database** from `y6a1c.exposures.positions.fits`\n",
    "\n",
    "---\n",
    "For running DES, obtain the databases using this notebook, run the command line arg `sorcha init` and select the **DES config file** option. Then run the code with:\n",
    "\n",
    "sorcha run -c DES_config_file.ini --pd DES_TNO.db --ob orbits_filename.csv -p colours_filename.csv -o ./ -s des --vd DES_visits.db\n",
    "\n",
    "`orbits_filename.csv` and `colours_filename.csv` should be your input files for object's orbits and colours, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70dc0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from sqlalchemy import create_engine\n",
    "import sqlite3\n",
    "from astropy.time import Time\n",
    "import pandas as pd\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import gzip\n",
    "\n",
    "\n",
    "\n",
    "def process_pointings_to_sqlite(\n",
    "    use_url,\n",
    "    fits_path=\"y6a1c.exposures.positions.fits\",\n",
    "    db_path=\"DES_TNO.db\",\n",
    "    url=\"https://github.com/bernardinelli/DESTNOSIM/raw/refs/heads/master/data/y6a1c.exposures.positions.fits\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes a FITS file containing DES pointing data and stores \n",
    "    data into a SQLite database with indexing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_url : bool\n",
    "        If True, the FITS file will be downloaded from the specified `url`. If False, the local file path\n",
    "        specified by `fits_path` will be used.\n",
    "\n",
    "    fits_path : str, optional\n",
    "        Path to the local FITS file (default is \"y6a1c.exposures.positions.fits\"). Only used if `use_url` is False.\n",
    "\n",
    "    db_path : str, optional\n",
    "        Path where the SQLite database will be saved (default is \"DES_TNO.db\").\n",
    "\n",
    "    url : str, optional\n",
    "        Direct URL to the raw FITS file hosted on GitHub.\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    if use_url == True:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with fits.open(BytesIO(response.content)) as hdul:\n",
    "            df = hdul[1].data\n",
    "    else:\n",
    "        with fits.open(fits_path) as HDUl:\n",
    "            df = HDUl[1].data\n",
    "\n",
    "    # Define column names\n",
    "    cov_1 = \"cov_xx\"\n",
    "    cov_2 = \"cov_yy\"\n",
    "    cov_3 = \"cov_xy\"\n",
    "    observatory_1 = \"observatory_1\"\n",
    "    observatory_2 = \"observatory_2\"\n",
    "    observatory_3 = \"observatory_3\"\n",
    "    velocity_1 = \"velocity_1\"\n",
    "    velocity_2 = \"velocity_2\"\n",
    "    velocity_3 = \"velocity_3\"\n",
    "\n",
    "    # Create dictionary for DataFrame construction\n",
    "    temp = {\n",
    "        cov_1: np.array(df[\"cov\"][:, 0], dtype=\"<f8\"),\n",
    "        cov_2: np.array(df[\"cov\"][:, 1], dtype=\"<f8\"),\n",
    "        cov_3: np.array(df[\"cov\"][:, 2], dtype=\"<f8\"),\n",
    "        \"covwarn\": df[\"covwarn\"],\n",
    "        \"fieldDec\": np.array(df[\"dec\"], dtype=\"<f8\"),\n",
    "        \"ecl_lat\": np.array(df[\"ecl_lat\"], dtype=\"<f8\"),\n",
    "        \"ecl_lon\": np.array(df[\"ecl_lon\"], dtype=\"<f8\"),\n",
    "        \"observationId\": np.array(df[\"expnum\"], \"<i4\"),\n",
    "        \"filter\": df[\"filter\"],\n",
    "        \"observationMidpointMJD\": np.array(df[\"mjd_mid\"], dtype=\"<f8\"),\n",
    "        observatory_1: np.array(df[\"observatory\"][:, 0], dtype=\"<f8\"),\n",
    "        observatory_2: np.array(df[\"observatory\"][:, 1], dtype=\"<f8\"),\n",
    "        observatory_3: np.array(df[\"observatory\"][:, 2], dtype=\"<f8\"),\n",
    "        \"obs_ecl_lon\": np.array(df[\"obs_ecl_lon\"], \"<f8\"),\n",
    "        \"fieldRA\": np.array(df[\"ra\"], dtype=\"<f8\"),\n",
    "        \"fiveSigmaDepth\": np.array(df[\"m50\"], dtype=\"<f8\"),\n",
    "        \"k\": np.array(df[\"k\"], dtype=\"<f8\"),\n",
    "        \"c\": np.array(df[\"c\"], dtype=\"<f8\"),\n",
    "        velocity_1: np.array(df[\"velocity\"][:, 0], dtype=\"<f8\"),\n",
    "        velocity_2: np.array(df[\"velocity\"][:, 1], dtype=\"<f8\"),\n",
    "        velocity_3: np.array(df[\"velocity\"][:, 2], dtype=\"<f8\"),\n",
    "    }\n",
    "\n",
    "    # Set exposure times\n",
    "    exo_time_s = np.full(len(df), 90)\n",
    "    mask = (temp[\"observationMidpointMJD\"] < 57447) & (temp[\"filter\"] == \"Y\")\n",
    "    exo_time_s[mask] = 45\n",
    "\n",
    "    # Convert TDB to TAI\n",
    "    time = Time(temp[\"observationMidpointMJD\"], format=\"mjd\", scale=\"tdb\")\n",
    "    time_TAI = time.tai\n",
    "    temp[\"observationMidpointMJD\"] = time_TAI.value\n",
    "\n",
    "    # Check for NaNs\n",
    "    if np.any(pd.isnull(temp[\"observationMidpointMJD\"])):\n",
    "        print(\"Warning: NaN values found in observationMidpointMJD\")\n",
    "\n",
    "    # Create DataFrame and insert exposure times\n",
    "    df_hdl1 = pd.DataFrame(temp)\n",
    "    df_hdl1.insert(8, \"visitExposureTime\", exo_time_s)\n",
    "\n",
    "    # Save to SQLite database\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    df_hdl1.to_sql(\"observations\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "    # Create indexes efficiently\n",
    "    index_queries = [\n",
    "        \"CREATE INDEX idx_lat_long ON observations(ecl_lat,ecl_lon)\",\n",
    "        \"CREATE INDEX idx_filter ON observations(filter)\",\n",
    "        \"CREATE INDEX idx_dec_ra ON observations(fieldDec,fieldRA)\",\n",
    "        \"CREATE INDEX idx_dec_ra_mjd ON observations(fieldDec,fieldRA,observationMidpointMJD)\",\n",
    "        \"CREATE INDEX idx_mjd ON observations(observationMidpointMJD)\",\n",
    "        \"CREATE INDEX idx_m50_c_k ON observations(fiveSigmaDepth,c,k)\"\n",
    "    ]\n",
    "\n",
    "    with sqlite3.connect(db_path) as db:\n",
    "        cursor = db.cursor()\n",
    "        for query in index_queries:\n",
    "            cursor.execute(query)\n",
    "\n",
    "    print(f\"DES pointing data processed and saved to: {db_path}\")\n",
    "\n",
    "\n",
    "\n",
    "def process_ccd_visits_to_sqlite(\n",
    "    use_url,\n",
    "    pointings_db_path,\n",
    "    fits_path=\"y6a1c.ccdcorners.fits.gz\",\n",
    "    db_path=\"DES_visits.db\",\n",
    "    url=\"https://github.com/bernardinelli/DESTNOSIM/raw/refs/heads/master/data/y6a1c.ccdcorners.fits.gz\"\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Processes a FITS file containing DES ccd visits and stores \n",
    "    data into a SQLite database with indexing.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    use_url : bool\n",
    "        If True, the FITS file will be downloaded from the specified `url`. If False, the local file path\n",
    "        specified by `fits_path` will be used.\n",
    "\n",
    "    fits_path : str, optional\n",
    "        Path to the local FITS file (default is \"y6a1c.ccdcorners.fits.gz\"). \n",
    "\n",
    "    db_path : str, optional\n",
    "        Path where the SQLite database will be saved (default is \"DES_TNO.db\").\n",
    "\n",
    "    url : str, optional\n",
    "        Direct URL to the raw FITS file hosted on GitHub.\n",
    "\n",
    "   \n",
    "    \"\"\"\n",
    "    if use_url == True:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with gzip.open(BytesIO(response.content), 'rb') as gz:\n",
    "                with fits.open(gz) as hdul:\n",
    "                    df = hdul[1].data\n",
    "    else:\n",
    "        with fits.open(fits_path) as HDUl:\n",
    "            df = HDUl[1].data\n",
    "\n",
    "\n",
    "    ra = df[\"ra\"]\n",
    "    dec = df[\"dec\"]\n",
    "\n",
    "    temp = {\n",
    "        \"visitId\": df[\"expnum\"],\n",
    "        \"detectorID\": df[\"ccdnum\"],\n",
    "        \"llcra\": ra[:, 0],\n",
    "        \"llcdec\": dec[:, 0],\n",
    "        \"lrcra\": ra[:, 1],\n",
    "        \"lrcdec\": dec[:, 1],\n",
    "        \"urcra\": ra[:, 2],\n",
    "        \"urcdec\": dec[:, 2],\n",
    "        \"ulcra\": ra[:, 3],\n",
    "        \"ulcdec\": dec[:, 3],\n",
    "        \"ra\": ra[:, 4],   # center RA\n",
    "        \"dec\": dec[:, 4], # center Dec\n",
    "    }\n",
    "\n",
    "    # Create DataFrame\n",
    "    df_hdl1 = pd.DataFrame(temp)\n",
    "    \n",
    "    engine_pointings = create_engine(f\"sqlite:///{pointings_db_path}\")\n",
    "    df_depth = pd.read_sql(\"SELECT observationId, fiveSigmaDepth FROM observations\", engine_pointings)\n",
    "    df_depth = df_depth.set_index(\"observationId\")\n",
    "    df_depth = pd.read_sql(\"SELECT observationId, fiveSigmaDepth FROM observations\", engine_pointings)\n",
    "    df_depth = df_depth.astype({\"fiveSigmaDepth\": \"<f8\"})  # just to be safe on dtype\n",
    "    df_hdl1[\"magLim\"] = df_hdl1[\"visitId\"].map(df_depth[\"fiveSigmaDepth\"])\n",
    "    \n",
    "\n",
    "    # Save to SQLite\n",
    "    engine = create_engine(f\"sqlite:///{db_path}\")\n",
    "    df_hdl1.to_sql(\"observations\", engine, if_exists=\"replace\", index=False, chunksize=10000)\n",
    "\n",
    "    # Create spatial index\n",
    "    index_sql = \"\"\"\n",
    "    CREATE INDEX IF NOT EXISTS idx_obs \n",
    "    ON observations(visitId, llcra, llcdec, lrcra, lrcdec, urcra, urcdec, ulcra, ulcdec)\n",
    "    \"\"\"\n",
    "\n",
    "    with sqlite3.connect(db_path) as db:\n",
    "        db.execute(index_sql)\n",
    "\n",
    "    print(f\"DES CCD visits data processed and saved to {db_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f09d3b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DES pointing data processed and saved to: DES_TNO.db\n",
      "DES CCD visits data processed and saved to DES_visits.db\n"
     ]
    }
   ],
   "source": [
    "pointing_db = \"DES_TNO.db\"\n",
    "process_pointings_to_sqlite(True,db_path=pointing_db)\n",
    "process_ccd_visits_to_sqlite(True,pointings_db_path=pointing_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115468ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sorcha",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
